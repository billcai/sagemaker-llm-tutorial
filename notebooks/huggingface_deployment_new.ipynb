{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9eee68-d5b3-4bec-8b4b-e6b3afce8077",
   "metadata": {},
   "source": [
    "# Deploying instruction-tuned LLM (Falcon 7B Instruct) on Amazon SageMaker\n",
    "\n",
    "In this tutorial, we deploy a recently released open-source large language models:\n",
    "\n",
    "1. Falcon 7B Instruct. This is a 7B Falcon model that was recently changed to use the permissive license Apache 2.0. [HF link](https://huggingface.co/tiiuae/falcon-7b-instruct)\n",
    "\n",
    "This notebook is tested on SageMaker Studio with Data Science 2.0 kernel and is meant to deploy a `ml.g4dn.xlarge` instance in the `ap-southeast-1` region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e7327-91b3-46ff-a255-a0566b8810b9",
   "metadata": {},
   "source": [
    "## Setup of IAM role\n",
    "We use the default SageMaker Studio execution role for the model execution role. Change this if you have a specific role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb7c31-e5a8-4a8f-9347-605674287a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c351a65b-729b-4d80-b8cc-070c43a81b40",
   "metadata": {},
   "source": [
    "## (Optional) Download and packaging of model code\n",
    "To run this example, we need to upgrade certain packages and change the inference.py file within the HuggingFace container. This requires packaging the model into a single zipped archive file. To speed up the tutorial, you can download the packaged zipped file with a provided link by me. If you want to package it directly, uncomment the next cell to install the necessary packages, download the original HF files, write the additional files and zip the folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deefece-6df3-4f6f-a5e5-39e303922b12",
   "metadata": {},
   "source": [
    "### Download HF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b11618-22e1-4f9e-808c-5508639dfaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Install HuggingFace Hub\n",
    "# !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e6b36-5341-4e7b-8c72-9fdb3dfcc877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Download Falcon 7B Instruct\n",
    "# from huggingface_hub import snapshot_download\n",
    "# from pathlib import Path\n",
    "# import shutil\n",
    "# model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "# model_tar_dir = Path(model_id.split(\"/\")[-1])\n",
    "# if model_tar_dir.exists():\n",
    "#     shutil.rmtree(str(model_tar_dir))\n",
    "# model_tar_dir.mkdir(exist_ok=True)\n",
    "# ignore_patterns = [\"*.msgpack\", \"*.h5\",\"*weight.bin\"]\n",
    "# # Download model from Hugging Face into model_dir\n",
    "# snapshot_download(model_id, local_dir=str(model_tar_dir), local_dir_use_symlinks=False,ignore_patterns=ignore_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451b129-a12d-4c12-aeb4-b7144cf471f1",
   "metadata": {},
   "source": [
    "### Add inference.py and requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e3ebb-ca90-4a2a-89c7-95f0c7a6db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create code directory\n",
    "# code_path = Path(\"./falcon-7b-instruct/code\")\n",
    "# code_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ccc6b-a3c9-46c6-a3a8-b085648fe409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile falcon-7b-instruct/code/inference.py\n",
    "# import torch\n",
    "# from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# def model_fn(model_dir):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "#     instruct_pipeline = pipeline(\n",
    "#         'text-generation',\n",
    "#         model=model_dir,\n",
    "#         torch_dtype=torch.bfloat16,\n",
    "#         trust_remote_code=True,\n",
    "#         device_map=\"auto\",\n",
    "#         tokenizer=tokenizer,\n",
    "#         model_kwargs={\"load_in_8bit\": True},\n",
    "#     )\n",
    "\n",
    "#     return instruct_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22be0bd-504a-4e02-9896-dee85f1c3e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile falcon-7b-instruct/code/requirements.txt\n",
    "# transformers==4.27.4\n",
    "# accelerate==0.18.0\n",
    "# bitsandbytes==0.38.1\n",
    "# einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf32686-724b-46f1-aefe-5c18942f76f9",
   "metadata": {},
   "source": [
    "### Package into zip and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a96f5-db6d-49f6-80ff-64c0a7aebe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get -y install pigz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3a215-27e3-4f60-9186-ddeb8733d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# parent_dir=os.getcwd()\n",
    "\n",
    "# # change to model dir\n",
    "# os.chdir(\"falcon-7b-instruct\")\n",
    "# # use pigz for faster and parallel compression\n",
    "# !tar -cf model.tar.gz --use-compress-program=pigz *\n",
    "# # change back to parent dir\n",
    "# os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f93b02-0f49-4a2c-84bf-29cdd6e36b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# sess = sagemaker.Session()\n",
    "# role = sagemaker.get_execution_role()\n",
    "# s3_model_uri = S3Uploader.upload(\n",
    "#     local_path=str(model_tar_dir.joinpath(\"model.tar.gz\")),\n",
    "#     desired_s3_uri=f\"s3://{sess.default_bucket()}/falcon-7b-instruct/{model_tar_dir}\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9762b1c-6b77-4ab5-983e-2a9273bcfbf1",
   "metadata": {},
   "source": [
    "## Start here with the pre-packaged tar file\n",
    "Comment out the cell below if you are packaging directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50051252-fc11-4eb7-be8b-5729fc7fd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_model_uri = S3Uploader.upload(\n",
    "    local_path=str(os.chdir().joinpath(\"model.tar.gz\")),\n",
    "    desired_s3_uri=f\"s3://{sess.default_bucket()}/falcon-7b-instruct/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97ef01-1b6d-441c-993d-15aace432051",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deployment of model\n",
    "Now we will deploy the model to Amazon SageMaker as a real-time endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0afe3-146a-4d13-af96-31c9bfb9ce7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import json\n",
    "\n",
    "use_quantization = True # whether to use quantization or not\n",
    "instance_type = \"ml.g4dn.xlarge\" # instance type to use for deployment\n",
    "number_of_gpu = 1 # number of gpus to use for inference and tensor parallelism\n",
    "health_check_timeout = 300 # Increase the timeout for the health check to 5 minutes for downloading the model\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  model_data=s3_model_uri,\n",
    "  role=role,\n",
    "  image_uri=\"763104351884.dkr.ecr.ap-southeast-1.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\",\n",
    "  model_server_workers=1,\n",
    "  env={\n",
    "    'HF_MODEL_QUANTIZE': json.dumps(use_quantization),\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu)\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909cc96-4341-4f8f-9244-5a701d809ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "endpoint_name = 'falcon-7b-instruct-{}'.format(str(uuid.uuid4()))\n",
    "predictor = llm_model.deploy(\n",
    "  endpoint_name=endpoint_name,\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8416ebfc-cab2-48cf-92ee-2f409a924618",
   "metadata": {},
   "source": [
    "## Test our deployed model directly with SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529951d9-6fbe-475f-8977-d2fc55a648fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = \"\"\"Summarize the following passage.\n",
    "Amazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment. \n",
    "It provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't have to manage servers. \n",
    "It also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment. \n",
    "With native support for bring-your-own-algorithms and frameworks, SageMaker offers flexible distributed training options that adjust to your specific workflows. \n",
    "Deploy a model into a secure and scalable environment by launching it with a few clicks from SageMaker Studio or the SageMaker console.<endoftext>\"\"\"\n",
    "\n",
    "parameters = {\n",
    "  \"max_new_tokens\":512,\"temperature\":0.7\n",
    "}\n",
    "\n",
    "# Run prediction\n",
    "result = predictor.predict({\n",
    "\t\"inputs\":payload,\n",
    "  \"parameters\" :parameters\n",
    "})\n",
    "print(result[0]['generated_text'].split('<endoftext>')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff4583-92b4-4085-92bf-bb145c2b5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = \"\"\"Write a Java program that reverses a string and duplicates it 10 times.<endoftext>\"\"\"\n",
    "\n",
    "parameters = {\n",
    "  \"max_new_tokens\":512,\"temperature\":0.7, \"do_sample\": True, \"top_p\":0.1, \"top_k\":50\n",
    "}\n",
    "\n",
    "# Run prediction\n",
    "result = predictor.predict({\n",
    "\t\"inputs\":payload,\n",
    "  \"parameters\" :parameters\n",
    "})\n",
    "print(result[0]['generated_text'].split('<endoftext>')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42cdce-f7e2-4232-be30-02fc8a2e811d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy your own Chatbot backed by Amazon SageMaker\n",
    "We use Gradio to deploy a simple chatbot backed by the model that you've just deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd5166-89c8-4544-8e3b-543971cc0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7cddb-0446-41e4-9e76-4e7253e9dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# hyperparameters for llm\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.1,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03\n",
    "  }\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Chat with Falcon 7B Instruct\")\n",
    "    with gr.Column():\n",
    "        chatbot = gr.Chatbot()\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                message = gr.Textbox(label=\"Chat Message Box\", placeholder=\"Chat Message Box\", show_label=False)\n",
    "            with gr.Column():\n",
    "                with gr.Row():\n",
    "                    submit = gr.Button(\"Submit\")\n",
    "                    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        # convert chat history to prompt\n",
    "        converted_chat_history = \"\"\n",
    "        if len(chat_history) > 0:\n",
    "          for c in chat_history:\n",
    "            converted_chat_history += f\"<|prompter|>{c[0]}<|endoftext|><|assistant|>{c[1]}<|endoftext|>\"\n",
    "        prompt = f\"{converted_chat_history}<|prompter|>{message}<|endoftext|><|assistant|>\"\n",
    "\n",
    "        # send request to endpoint\n",
    "        llm_response = predictor.predict({\"inputs\": prompt, \"parameters\": parameters})\n",
    "\n",
    "        # remove prompt from response\n",
    "        parsed_response = llm_response[0][\"generated_text\"][len(prompt):]\n",
    "        if 'User' in parsed_response:\n",
    "            parsed_response.split('User')[0]\n",
    "        if '<|endoftext|>' in parsed_response:\n",
    "            parsed_response.split('<|endoftext|>')[0]\n",
    "        chat_history.append((message, parsed_response))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    submit.click(respond, [message, chatbot], [message, chatbot], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
