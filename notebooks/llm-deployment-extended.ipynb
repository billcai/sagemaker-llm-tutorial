{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e5cb506-ddda-4b0d-95b1-70404e606039",
   "metadata": {},
   "source": [
    "# Extended Examples on Deploying LLMs on SageMaker\n",
    "\n",
    "Examples included in this notebook:\n",
    "1. Packaging and deploying from S3 compressed artifacts\n",
    "2. Deploying in network isolation mode\n",
    "3. Deploying with Deep Java Library and DeepSpeed (requires SageMaker instance types with num of GPUs >= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed33ed5-0634-4546-b7a3-c26c8a8a813d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.140.0\" boto3 \"huggingface_hub==0.13.0\" \"hf-transfer\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b44f8-c0de-4337-b93d-d6c9a15bcd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install -y pigz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd3e9d-2085-40a5-b8d8-ba40ce0397e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packaging and deploying from S3 compressed artifacts\n",
    "\n",
    "You can skip the packaging steps in the interest of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448673b2-fad4-48ff-9216-acc387ed0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# set HF_HUB_ENABLE_HF_TRANSFER env var to enable hf-transfer for faster downloads\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID=\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n",
    "# create model dir\n",
    "model_tar_dir = Path(HF_MODEL_ID.split(\"/\")[-1])\n",
    "model_tar_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download model from Hugging Face into model_dir\n",
    "snapshot_download(HF_MODEL_ID, local_dir=str(model_tar_dir), local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f64858-40bb-46e2-89dc-996c0bc082ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir=os.getcwd()\n",
    "# change to model dir\n",
    "os.chdir(str(model_tar_dir))\n",
    "# use pigz for faster and parallel compression\n",
    "!tar -cf oasst-pythia-12b.tar.gz --use-compress-program=pigz *\n",
    "# change back to parent dir\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0430a9-a076-47e6-bd9c-195f704e583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp oasst-sft-4-pythia-12b-epoch-3.5/oasst-pythia-12b.tar.gz s3://sagemaker-us-west-2-224755080010/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe0fa5-d691-47c0-9843-7d2f2da9165d",
   "metadata": {},
   "source": [
    "### Deployment with Text Generation Inference \n",
    "\n",
    "We use a modified version of HuggingFace's [Text Generation Inference](https://github.com/huggingface/text-generation-inference). We modified the `v0.4.0` with the following `Dockerfile` and `sagemaker-entrypoint.sh`.\n",
    "\n",
    "`Dockerfile`:\n",
    "```\n",
    "FROM 224755080010.dkr.ecr.us-west-2.amazonaws.com/sagemaker-text-generation-inference:0.4\n",
    "COPY sagemaker-entrypoint.sh entrypoint.sh\n",
    "RUN chmod +x entrypoint.sh\n",
    "\n",
    "ENTRYPOINT [\"./entrypoint.sh\"]\n",
    "```\n",
    "\n",
    "`sagemaker-entrypoint.sh`:\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "if [[ -z \"${HF_MODEL_ID}\" ]]; then\n",
    "  echo \"HF_MODEL_ID must be set\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "if [[ -n \"${HF_MODEL_REVISION}\" ]]; then\n",
    "  export REVISION=\"${HF_MODEL_REVISION}\"\n",
    "fi\n",
    "\n",
    "if [[ -n \"${SM_NUM_GPUS}\" ]]; then\n",
    "  export NUM_SHARD=\"${SM_NUM_GPUS}\"\n",
    "fi\n",
    "\n",
    "if [[ -n \"${HF_MODEL_QUANTIZE}\" ]]; then\n",
    "  export QUANTIZE=\"${HF_MODEL_QUANTIZE}\"\n",
    "fi\n",
    "\n",
    "text-generation-launcher --port 8080 --model-id $HF_MODEL_ID\n",
    "```\n",
    "\n",
    "This Docker image is hosted at `224755080010.dkr.ecr.us-west-2.amazonaws.com/sagemaker-text-generation-inference:0.4-mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d18804-c626-45d2-be3f-26ed6f7f4970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95431ec8-47e8-445b-9772-9ba96843ebea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.model import Model\n",
    "import sagemaker\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "hf_model_id = \"/opt/ml/model/\" # change to model directory where SageMaker model is loaded\n",
    "use_quantized_model = True # whether to use quantization or not\n",
    "instance_type = \"ml.g5.xlarge\"\n",
    "number_of_gpu = 1 # number of gpus to use for inference and tensor parallelism\n",
    "\n",
    "health_check_timeout = 1800 # Increase the timeout for the health check to 15 minutes for downloading model\n",
    "\n",
    "# account_id = sess.account_id()\n",
    "account_id= \"224755080010\"\n",
    "region =  sagemaker.Session(boto_session=boto3.Session(region_name='us-west-2')).boto_region_name\n",
    "\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/sagemaker-text-generation-inference:0.4-mod\".format(\n",
    "    account_id,region)\n",
    "\n",
    "hf_model = Model(\n",
    "  role=role,\n",
    "  model_data=\"s3://sagemaker-us-west-2-224755080010/oasst-pythia-12b.tar.gz\",\n",
    " sagemaker_session=sagemaker.Session(boto_session=boto3.Session(region_name='us-west-2')),\n",
    "  image_uri=image_uri,\n",
    "  env={\n",
    "    'HF_MODEL_ID':hf_model_id,\n",
    "    'HF_MODEL_QUANTIZE': json.dumps(use_quantized_model),\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu),\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48de78-6363-4af4-8e2a-68585f3f5a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "endpoint_name = 'oa-pythia-s3-{}'.format(str(uuid.uuid4()))\n",
    "hf_model.deploy(\n",
    "  endpoint_name=endpoint_name,\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895e51d-04e0-4c08-a61d-37ac5b0be643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFacePredictor\n",
    "predictor = HuggingFacePredictor(endpoint_name=endpoint_name,\n",
    "                                 sagemaker_session=sagemaker.Session(boto_session=boto3.Session(region_name='us-west-2')))\n",
    "payload = \"\"\"\n",
    "<|prompter|>Summarize the following passage.\n",
    "Amazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment. \n",
    "It provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't have to manage servers. \n",
    "It also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment. \n",
    "With native support for bring-your-own-algorithms and frameworks, SageMaker offers flexible distributed training options that adjust to your specific workflows. \n",
    "Deploy a model into a secure and scalable environment by launching it with a few clicks from SageMaker Studio or the SageMaker console.<|endoftext|><|assistant|>\"\"\"\n",
    "\n",
    "parameters = {\n",
    "  \"max_new_tokens\":512,\"temperature\":0.7,\"do_sample\":True,\"top_k\":40,\"top_p\":0.1\n",
    "}\n",
    "\n",
    "# Run prediction\n",
    "predictor.predict({\n",
    "\t\"inputs\": payload,\n",
    "  \"parameters\" :parameters\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99e961-eba4-4036-8353-7b6476fac1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9a617-f69b-4446-af1f-54b5e253de68",
   "metadata": {},
   "source": [
    "## Deploying with network isolation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96efc067-cae7-47d1-a944-bd27c1bee978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.model import Model\n",
    "import sagemaker, boto3\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "hf_model_id = \"/opt/ml/model/\" # change to model directory where SageMaker model is loaded\n",
    "use_quantized_model = True # whether to use quantization or not\n",
    "instance_type = \"ml.g5.xlarge\"\n",
    "number_of_gpu = 1 # number of gpus to use for inference and tensor parallelism\n",
    "\n",
    "health_check_timeout = 1800 # Increase the timeout for the health check to 15 minutes for downloading model\n",
    "\n",
    "# account_id = sess.account_id()\n",
    "account_id= \"224755080010\"\n",
    "region =  sagemaker.Session(boto_session=boto3.Session(region_name='us-west-2')).boto_region_name\n",
    "\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/sagemaker-text-generation-inference:0.4-mod-new\".format(\n",
    "    account_id,region)\n",
    "\n",
    "hf_model = Model(\n",
    "  role=role,\n",
    "  model_data=\"s3://sagemaker-us-west-2-224755080010/oasst-pythia-12b.tar.gz\",\n",
    " sagemaker_session=sagemaker.Session(boto_session=boto3.Session(region_name='us-west-2')),\n",
    "  image_uri=image_uri,\n",
    "  env={\n",
    "    'HF_MODEL_ID':hf_model_id,\n",
    "    'HF_MODEL_QUANTIZE': json.dumps(use_quantized_model),\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu),\n",
    "  },\n",
    "  enable_network_isolation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8966cda-566d-4142-952c-75fabb4ef581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "endpoint_name = 'oa-pythia-s3-{}'.format(str(uuid.uuid4()))\n",
    "hf_model.deploy(\n",
    "  endpoint_name=endpoint_name,\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf069c-5ce8-4b22-8222-1b036410722f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFacePredictor\n",
    "predictor = HuggingFacePredictor(endpoint_name=endpoint_name,\n",
    "                                 sagemaker_session=sagemaker.Session(boto_session=boto3.Session(region_name='us-west-2')))\n",
    "payload = \"\"\"\n",
    "<|prompter|>Summarize the following passage.\n",
    "Amazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment. \n",
    "It provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't have to manage servers. \n",
    "It also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment. \n",
    "With native support for bring-your-own-algorithms and frameworks, SageMaker offers flexible distributed training options that adjust to your specific workflows. \n",
    "Deploy a model into a secure and scalable environment by launching it with a few clicks from SageMaker Studio or the SageMaker console.<|endoftext|><|assistant|>\"\"\"\n",
    "\n",
    "parameters = {\n",
    "  \"max_new_tokens\":512,\"temperature\":0.7,\"do_sample\":True,\"top_k\":40,\"top_p\":0.1\n",
    "}\n",
    "\n",
    "# Run prediction\n",
    "predictor.predict({\n",
    "\t\"inputs\": payload,\n",
    "  \"parameters\" :parameters\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8d89b-3506-4765-a147-240f7b8547a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b193159-073c-4697-abf5-c4ee23515359",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploying with Deep Java Library (DJL) and DeepSpeed\n",
    "[DeepSpeed](https://github.com/microsoft/DeepSpeed) is a library developed to optimize deep learning model training and inference. [DJL](https://github.com/deepjavalibrary/djl-serving) is a model deployment library intended for deep learning models. Based on HuggingFace's own benchmark [https://huggingface.co/blog/bloom-inference-pytorch-scripts], DeepSpeed allows for high throughput inference that can distribute model weights that may not fit within a single GPU across multiple GPUs.\n",
    "\n",
    "SageMaker's Python SDK is integrated with DJL and DeepSpeed. Check out the full documentation [here](https://sagemaker.readthedocs.io/en/stable/frameworks/djl/using_djl.html#inference-code-and-model-server-properties). \n",
    "\n",
    "For this tutorial, you will need access to instances with >= 2 GPUs. We will use `g4dn.12xlarge` to deploy [OpenChatKit](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B), a new (but relatively older) GPT Neo-X 20B model that is tuned on an instruction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b6996-509c-4744-a58e-5a512bdecf14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77ff69-36be-493f-9cbb-9659108145cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.djl_inference.model import DeepSpeedModel\n",
    "\n",
    "model = DeepSpeedModel(\n",
    "    \"togethercomputer/GPT-NeoXT-Chat-Base-20B\",\n",
    "    role,\n",
    "    tensor_parallel_degree=2,\n",
    "    task=\"text-generation\",\n",
    "    data_type=\"int8\",\n",
    ")\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.g4dn.12xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3cb56-ab46-4539-bfcc-08ce3c224402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bee598-21e8-4d35-b69b-87579f555272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
